{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curse of Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Many Machine Learning problems have thousands of features for even training instance.Not only does this make training slow it can also make it much harder to find a good solution,as we will see.This problem is often referred as*** Curse Of Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Fortunately,in real-world problems it is often possible reduce the number of features like Removing the border pixels of a image as they are always white or you can merge adjacent pixles as they will be having same pixel intensity***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'images/pixel.png' width = 400px>\n",
    "\n",
    "***Reducing Dimensinolity does lose some information like compressing JPEG will degrade it quality.It also make pipelines a bit more complex and harder to maintain.sometimes Reducing dimensions may result in lessen the training time ,reduce noise and may increase performance But it Rarely Happens***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Apart from Speeding up training,Dimensinonality reduction is also useful for data visulization.Reducing tht dimensions two or three makes it possible to plot a high-dimensional training set on a graph and often gain some important insights by visually detecting patterns such as clusters***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Two Main Approaches for Dimensionality Reductions </h3>\n",
    "<ul><b><i><li>Projection</li>\n",
    "    <li>Manifold</li></b></i>\n",
    "</ul>\n",
    "<h3>Most Popular Dimensionality Reduction techniques</h3>\n",
    "<ul><b><i>\n",
    "    <li>Principal Component Analysis</li>\n",
    "    <li>Kernal PCA</li>\n",
    "    <li>LLE</li></b></i>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src ='images/dim.png' width=500px>\n",
    "<h5><center>point,segment,square,cube ,tesseract</center></h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Many things behave differntly in high-diemensional space.For Example if you pick a random point in a unit square,it will only have 0.4% chance of being located less than 0.0001 from a border i.e.,it is unlikely that a random point will be 'extreme' along any dimension.But in 10,000 dimension unit hypercube this probability is greater than 99.9999%.Most points in high-dimensional hypercube are very close to border***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ***Here is a most troublesome differnence,if you pick any two random points in a unit square the distance between those points will be roughly on average be 0.52 ,if those two points are in a 3D cube then it will be roughly 0.66,But in 10000 dimensional space it will be 408.25.This is quite counterintitve:how two points in a unit cube be that much.The fact is high-dimensional datasets are at risk being very parse:most training instances are likely to be far away from each other.Ofcouse this also means that new instance will likely to be far away from training instance ,making predictions much less reliable than in lower dimensions,since they will be based on much larger extrapolations.In short,More dimensions the training set has ,the greater the risk of overfitting it.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***In Theory,one solution of the curse of dimensionality could be to increase size of training set to reach a sufficient density of training instances required to reach a sufficent density of training instances.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Projection\n",
    "***In most real world problems,the training instances are not spread uniformly across all dimensions.Many features are almost constant,while others are highly corelated.As a result,all training instances lie within or close to much lower dimensional subspace of higher dimensional space***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src ='images/pro.png' width = 450px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Notice that all points are close to a plane:lower dimension 2D of higher Dimension 3D space.Now we project  every training instance perpendicularly onto this subspace,we get 2D dataset.Note that axes correspond to new features $z1$ and $z2$ are shown*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'images/new.png' width =400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***However Projection may not always be the best approach since sometimes the subspace may twist and turn such as in famous Swiss toy roll dataset***\n",
    "<img src ='images/toy.png' width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Simply Projecting on to plane (by dropping $x3$) would squash different layers of the swiss roll together as shown.But what you want to is to unroll the Swiss roll to obtain the 2D data set***\n",
    "<img src = 'images/toy2.png' width=700px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manifold Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The swiss roll is 2D manifold.Simply a 2D Manifold is 2D Shape which can be bent and twisted in a higher dimensinonal space.More generally a d-Manifold is a part of n-dimensional space(d<n) that locally resembles a d-dimensional hyperplane.In this case of swiss roll ,d=2,n=3:it locally resembles a 2D plane but it is rolled in the third dimension.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Many Dimensionality Reduction Algorithms work by modelling manifold on which training instances lie,this is called Manifold Learning.It relies on manifold assumptions called manifold hypothesis,which hold that most real world high-dimension datasets lie close to much lower -dimensional manifold.This assumption is empiraclly observed***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
