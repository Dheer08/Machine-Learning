{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***PCA is far most popular dimensionality reduction algorithm.First,it identifies the hyperplane that lies closest to the data,and then it projects the data onto it***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The important thing is to choose right hyperplane which preserves variance the most***\n",
    "<img src ='images/hyper.png' width = 450px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***As you can see, the projection onto the solid line preserves the maximum\n",
    "variance, while the projection onto the dotted line preserves very little variance, and\n",
    "the projection onto the dashed line preserves an intermediate amount of variance.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***It Seems Resonable to select the axis that preserves the maximum amount of variance ,as it most likely lose less information than the other projections.Another way to justify the choice is that it it axis that minimizes the mean squared distance between the original dataset and its projection onto that axis.This is rather simple idea behind PCA.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***PCA identifies the axis that accounts for the largest amount of variance in the training set. In Figure above, it is the solid line. It also finds a second axis, orthogonal to the\n",
    "first one, that accounts for the largest amount of remaining variance. In this 2D\n",
    "example there is no choice: it is the dotted line. If it were a higher-dimensional data‐\n",
    "set, PCA would also find a third axis, orthogonal to both previous axes, and a fourth,\n",
    "a fifth, and so on—as many axes as the number of dimensions in the dataset.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The unit vector that defines the i th axis is called the i th principal component (PC). In\n",
    "Figure above, the 1 st PC is c 1 and the 2 nd PC is c 2 . In Figure above the first two PCs are\n",
    "represented by the orthogonal arrows in the plane, and the third PC would be\n",
    "orthogonal to the plane (pointing up or down).***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***So how can you find the principal components of a training set? Luckily, there is a\n",
    "standard matrix factorization technique called Singular Value Decomposition (SVD)\n",
    "that can decompose the training set matrix X into the dot product of three matrices U\n",
    "· Σ ·$ V^T $, where $V^T$ contains all the principal components that we are looking for.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The following Python code uses NumPy’s svd() function to obtain all the principal\n",
    "components of the training set, then extracts the first two PCs***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X =np.array([[1,2,3],[3,4,5]])\n",
    "X_centered = X - X.mean(axis=0)\n",
    "U, s, V = np.linalg.svd(X_centered)\n",
    "c1 = V.T[:, 0]\n",
    "c2 = V.T[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Using Sckit-learn***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00000000e+00 2.05432527e-33]\n"
     ]
    }
   ],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
